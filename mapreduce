__author__ = 'Xavi Domenech & Manuel Rojo'

import string
import multiprocessing


def Map(L):
    return None


def Reduce(Mapping):
    return None


def uniformTest(size):
    p = [multiprocessing.Pool(1) for i in range(size)]
    for pool in p:
        pool.terminate()
        pool.join()


def randomTest(size):
    p = multiprocessing.Pool(size)
    p.terminate()
    p.join()


def num_lines_file(filename, num_processes):
    """
    :param filename: Name of the file to count
    :param num_processes: Number of processes to do the map/reduce
    :return: Number of lines to process for each process
    """
    lines_process = []

    num_lines = sum(1 for line in open(filename)) # Count number of lines in the file

    rest = num_lines % num_processes

    for i in range(0, num_processes):
        if i == num_processes - 1:
            lines_process.append(int(num_lines / num_processes) + rest)
        else:
            lines_process.append(int(num_lines / num_processes))
    print lines_process
    return lines_process


def chunks(filename, num_processes):
    """
    :param filename: Name of the file to count
    :param num_processes: Number of processes to do the map/reduce
    :return: A matrix [][] with the lines to process for each process
    """
    chunk = []
    chunks = []
    file = open(filename, 'rb')
    for lines in num_lines_file(filename, num_processes):
        chunk = []
        for each_line in range(0, lines):
            chunk.append(file.readline())
        chunks.append(chunk)
    return chunks


def map_reduce():
    return None


if __name__ == "__main__":
    chunks('file/logs.txt', 4)
    map_reduce()
