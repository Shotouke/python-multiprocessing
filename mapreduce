__author__ = 'Xavi Domenech & Manuel Rojo'

import string
import multiprocessing


def Map(L):
    print multiprocessing.current_process().name
    result = {}
    for i, datum in enumerate(L):
        a=string.split(datum, '	')
        #key=(ip,month,year)
        key=(a[0],a[3],a[5]);
        if result.has_key(key)==False:
            result[key]=[]
        result[key].append(1)
    #print result
    return result


def Reduce(Mapping):

    return  sum( Mapping)


def uniformTest(size):
    p = [multiprocessing.Pool(1) for i in range(size)]
    for pool in p:
        pool.terminate()
        pool.join()


def randomTest(size):
    p = multiprocessing.Pool(size)
    p.terminate()
    p.join()


def num_lines_file(filename, num_processes):
    """
    :param filename: Name of the file to count
    :param num_processes: Number of processes to do the map/reduce
    :return: Number of lines to process for each process
    """
    lines_process = []

    num_lines = sum(1 for line in open(filename)) # Count number of lines in the file

    rest = num_lines % num_processes

    for i in range(0, num_processes):
        if i == num_processes - 1:
            lines_process.append(int(num_lines / num_processes) + rest)
        else:
            lines_process.append(int(num_lines / num_processes))
    print lines_process
    return lines_process


def chunks(filename, num_processes):
    """
    :param filename: Name of the file to count
    :param num_processes: Number of processes to do the map/reduce
    :return: A matrix [][] with the lines to process for each process
    """
    chunks = []
    file = open(filename, 'rb')
    for lines in num_lines_file(filename, num_processes):
        chunk = []
        for each_line in range(0, lines):
            chunk.append(file.readline())
        chunks.append(chunk)
    return chunks


def map_reduce():
    return None

def Partition(mapping):
    newDict={}
    for i in mapping:
        dict=i[0]
        for key in iter(dict):
            if newDict.has_key(key)==False:
                newDict[key]=dict[key]
            else:
                newDict[key].extend(dict[key])
    return newDict





if __name__ == "__main__":
    pieces=chunks('logs.txt', 4)
    p = [multiprocessing.Pool(1) for i in range(4)]
    mapping=[]
    for i, datum in enumerate(pieces): # Show the process and the piece
        mapping.append( p[i%4].map(Map, (datum,)))
    mapping=Partition(mapping)

    newMapping={}
    for key in iter(mapping): # Show the process and the piece
        newMapping[key]= p[i%4].apply(Reduce, [mapping[key]])
        print key
        print newMapping[key]
        print '\n'

