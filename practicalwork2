__author__ = 'Xavier Domenech & Manuel Rojo'

import py_ecc.file_ecc
import os
import random
import multiprocessing
import array
from os import getpid


def worker(data):
    """
    This worker saves the block passed as parameter
    """
    multiprocessing.current_process().__setattr__('data',data)
    return data


def worker2(data):
    """
    This worker returns the block
    """
    return multiprocessing.current_process().__getattribute__('data')


def is_alive():
    """
    This worker returns if process is alive
    """
    return multiprocessing.current_process().is_alive()


def name():
    """
    This worker returns the name of the process
    """
    return multiprocessing.current_process().name


def who_i_am(data):
    """
    The job of this worker is simply tell who it is ;-)
    """
    print "Hi! I'm {} and process {}. I'm processing {}!".format(multiprocessing.current_process().name, getpid(), data)


def fail_workers(pool, failures):
    """
    This function emulates failing nodes/processes by terminating the
    number of "failures" processes from the "pool".
    """
    if failures > len(pool):
        raise Exception("You want to fail {} workers from a total of {}, but you can't!!".format(failures, pool._processes))

    ids = random.sample(range(len(pool)), failures)
    num = 0
    for i in ids:
        "emulating a worker fails via its terminate()"
        pool[i].terminate()
        pool[i].join()
        num += 1
    print str(num) + " processes killed"


def choose_pieces(pool, size, recover):
    """
    Method to select a random stack of pieces to reconstruct
    the original data.

    pool => Pool of current processes
    size => Number of processes (number of file parts).
    recover => Number of minimum parts to recover the original data.
    """
    names = []
    alive = []
    pieces = []
    num_actives = 0
    actives = []
    for i in range(len(pool)):
        if pool[i]._state != 2:
            names.append(pool[i%size].apply(name))
            alive.append(pool[i%size].apply(is_alive))
            actives.append(i)
            num_actives += 1

    if (len(actives) >= recover):
        while (len(pieces) < recover):
            if(len(pieces)== 0):
                pieces.append(random.choice(actives))
            possiblePiece = random.choice(actives)
            valid = True
            for x in range(len(pieces)):
                if (pieces[x] == possiblePiece):
                    valid = False
            if valid:
                pieces.append(possiblePiece) # Random selection of pieces (Always minimum number of needed pieces)
        return pieces
    else:
        return "Null"

def study(size, pool, testFile, pieces,  prefix):
    """
    Method to calculate all the storage costs

    size => Number of processes (number of file parts).
    pool => Pool of processes used
    testFile => A reasonable size file for testing.
    pieces => List of choose pieces
    prefix => Name of the pieces to choose.
    """
    print "========== ANALYSIS =========="
    fileSize = os.path.getsize(testFile) # File size
    pieceSize = os.path.getsize(prefix + '.p_' + str(0)) # Piece size

    collaboration = []
    recived = []
    for i in range (size):
        recived.append(1)
        collaboration.append(0)

    total_collaboration = 0
    for i in range (len(pieces)):
        collaboration[pieces[i]] = 1
        total_collaboration += 1

    names = []
    for i in range(len(pool)):
        if pool[i]._state != 2:
            names.append(pool[i%size].apply(name))

    print "Storage cost ratio: " + str(float(pieceSize*size)/fileSize) + "x"
    print "Additional storage cost ratio: " + str(float(pieceSize*size)/fileSize-1) + "x"
    for i in range(len(pieces)):
        print "Collaboration ratio per process " + str(names[i]) + " => " + str(collaboration[pieces[i]]) + ":" + str(recived[pieces[i]]) + " => " + str(collaboration[pieces[i]]/recived[pieces[i]])
    print "Total collaboration ratio => " + str(total_collaboration)
    print "Correcting bits ratio: " + str(float(pieceSize*size)/fileSize-1) + "x"
    print "=============================="


def test_reed_salomon_elasticity(size, recover, fail, testFile, decodedFile):
    """
    Method to do the test for the practical work.
    First practice part => fail = 0
    Second practice part => fail = number of fail processes
    Distribute the pieces in different processes.

    size => Number of processes (number of file parts).
    recover => Number of minimum parts to recover the original data.
    fail => Number of processes that we want fail
    testFile => A reasonable size file for testing.
    decodedFile => Name of the output file after the recovery
    """
    print "### Running test_reed_salomon_elasticity with {} processes, {} processes to recover and {} processes failed".format(size, recover, fail)
    p = [multiprocessing.Pool(1) for i in range(size)]
    prefix = testFile + '_backup'
    names = py_ecc.file_ecc.EncodeFile(testFile,prefix,size,recover)

    for i, datum in enumerate(names):
        p[i%size].apply(who_i_am, (datum,))

    for i, datum in enumerate(names):
        p[i%size].apply(worker, (datum,))

    fail_workers(p, fail)
    recov = array.array('i',(0,)*recover)
    pieces = choose_pieces(p, size, recover)

    if pieces != "Null":
        decList = []
        for i, datum in enumerate(recov): # Save the pieces to recover for decode the file
            decList.append(p[pieces[i]%size].apply(worker2, (datum,)))

        py_ecc.file_ecc.DecodeFiles(decList,decodedFile) # Decode the file
    else:
        print "It's not possible decode data to recover the original file"

    # Study
    if pieces != "Null":
        study(size, p, testFile, pieces, prefix)

    for pool in p:
        pool.terminate()
        pool.join()


if __name__ == "__main__":

    # First part of the practical work: Reed Salomon for storage (No fails)

    test_reed_salomon_elasticity(8, 7, 0, 'tmp/testA1', 'tmp/testA1.r') # first test with erasure codes 8,7
    test_reed_salomon_elasticity(8, 4, 0, 'tmp/testA2', 'tmp/testA2.r') # second test with erasure codes 8,4
    test_reed_salomon_elasticity(8, 2, 0, 'tmp/testA3', 'tmp/testA3.r') # third test with erasure codes 8,2

    # Second part of the practical work: Elasticity and Reed Salomon (Fails)

    # Erasure code (8,7)
    test_reed_salomon_elasticity(8, 7, 1, 'tmp/testB1', 'tmp/testB1.r') # first test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 7, 2, 'tmp/testB1', 'tmp/testB1.r') # first test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 7, 3, 'tmp/testB1', 'tmp/testB1.r') # first test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 7, 4, 'tmp/testB1', 'tmp/testB1.r') # first test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 7, 5, 'tmp/testB1', 'tmp/testB1.r') # first test with erasure codes with elasticity

    # Erasure code (8,4)
    test_reed_salomon_elasticity(8, 4, 1, 'tmp/testB2', 'tmp/testB2.r') # second test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 4, 2, 'tmp/testB2', 'tmp/testB2.r') # second test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 4, 3, 'tmp/testB2', 'tmp/testB2.r') # second test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 4, 4, 'tmp/testB2', 'tmp/testB2.r') # second test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 4, 5, 'tmp/testB2', 'tmp/testB2.r') # second test with erasure codes with elasticity

    # Erasure code (8,2)
    test_reed_salomon_elasticity(8, 2, 1, 'tmp/testB3', 'tmp/testB3.r') # third test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 2, 2, 'tmp/testB3', 'tmp/testB3.r') # third test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 2, 3, 'tmp/testB3', 'tmp/testB3.r') # third test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 2, 4, 'tmp/testB3', 'tmp/testB3.r') # third test with erasure codes with elasticity
    test_reed_salomon_elasticity(8, 2, 5, 'tmp/testB3', 'tmp/testB3.r') # third test with erasure codes with elasticity
