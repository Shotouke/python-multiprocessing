__author__ = 'Xavier Domenech & Manuel Rojo'

import py_ecc.file_ecc
import multiprocessing
import random


def test_erasure_codes( size, recover, pieces, testFile, decodedFile):
    """
    Method to do the test for the first part of the practical work
    here we can code the original into n pieces, and then we recover it.

    size => Number of processes (number of file parts).
    recover => Number of minimum parts to recover the original data.
    pieces => Array of pieces to recover the original data.
    testFile => A reasonable size file for testing.
    decodedFile => Name of the output file after the recovery
    """

    prefix = testFile + '_backup'
    names = py_ecc.file_ecc.EncodeFile(testFile,prefix,size,recover) # Encode the file
    decList = map(lambda x: prefix + '.p_' + `x`, pieces) # List of files to decode
    py_ecc.file_ecc.DecodeFiles(decList,decodedFile) # Decode the file
    fd1 = open(testFile,'rb')
    fd2 = open(decodedFile,'rb')
    fd1.read() == fd2.read() # Compare if original and recovered file are equals


def who_i_am(data):
    """
    The job of this worker is simply tell who it is ;-)
    """
    print "Hi! I'm {} and I'm processing {}!".format(multiprocessing.current_process().name, data)


def test_pool_who_i_am_uniform(size, data):
    """
    This test forces a uniform distribution of workload among processes.
    To do so, we implement a pool of Pools for simplicity.
    """
    print "### Running pool test for uniform distribution of workload"
    p = [multiprocessing.Pool(1) for i in range(size)]

    "this time, we don't expect any result from the workers."
    "p.map(who_i_am, data)"
    for i, datum in enumerate(data):
        p[i%size].apply(who_i_am, (datum,))

    for pool in p:
        pool.terminate()
        pool.join()
    print ""


def fail_workers(pool, failures):
    """
    This function emulates failing nodes/processes by terminating the
    number of "failures" processes from the "pool".
    """
    if failures > pool._processes:
        raise Exception("You want to fail {} workers from a total of {}, but you can't!!".format(failures, pool._processes))

    ids = random.sample(range(pool._processes), failures)
    for i in ids:
        "emulating a worker fails via its terminate()"
        pool._pool[i].terminate()
        pool._pool[i].join()

    "after failing processes, we need to recover the amount of processes in the pool"
    pool._maintain_pool()


def choosePieces(size, recover):
    """
    Method to select a random stack of pieces to reconstruct
    the original data.

    size => Number of processes (number of file parts).
    recover => Number of minimum parts to recover the original data.
    """
    pieces = []
    while (len(pieces) < recover):
        if(len(pieces)== 0):
            pieces.append(random.randint(0, size-1))
        posiblePiece = random.randint(0, size-1)
        valid = True
        for x in range(len(pieces)):
            if (pieces[x] == posiblePiece):
                valid = False
        if (valid):
            pieces.append(posiblePiece) # Random selection of pieces (Always minimum number of needed pieces)
    return pieces


def test_erasure_codes_elasticity(size, recover, testFile, decodedFile):
    """
    Method to do the test for the second part of the practical work
    here we can distribute the pieces in different processes

    size => Number of processes (number of file parts).
    recover => Number of minimum parts to recover the original data.
    testFile => A reasonable size file for testing.
    decodedFile => Name of the output file after the recovery
    """

    print "### Running test_erasure_code_elasticity with {} processes".format(size)
    p = multiprocessing.Pool(size)
    prefix = testFile + '_backup'
    names = py_ecc.file_ecc.EncodeFile(testFile,prefix,size,recover)

    test_pool_who_i_am_uniform(size, names) # Assign a part of data to each process

    decList = map(lambda x: prefix + '.p_' + `x`,choosePieces(size, recover)) # List of files to decode



    py_ecc.file_ecc.DecodeFiles(decList,decodedFile) # Decode the file

    """ returnedData = p.map(worker2, data)
    for i in range(len(data)):
        print data[i]
        print returnedData[i]
        print ""
    """
    p.terminate()
    p.join()


if __name__ == "__main__":

    # First part of the practical work: Reed Salomon for storage

    test_erasure_codes(8, 2, [1,2], 'tmp/testA1', 'tmp/testA1.r') # first test with erasure codes 8,2
    test_erasure_codes(8, 4, [1,2,3,4], 'tmp/testA2', 'tmp/testA2.r') # second test with erasure codes 8,4
    test_erasure_codes(8, 7, [1,2,3,4,5,6,7], 'tmp/testA3', 'tmp/testA3.r') # third test with erasure codes 8,7

    # Second part of the practical work: Elasticity and Reed Salomon

    test_erasure_codes_elasticity(8, 4, 'tmp/ls', 'tmp/ls.r')
