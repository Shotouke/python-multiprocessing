import py_ecc.file_ecc
import test
import multiprocessing
import random
import os


def worker(data):
    """
    This worker returns the data passed as parameter
    """
    print multiprocessing.current_process().name
    print multiprocessing.current_process().is_alive()
    if multiprocessing.current_process().is_alive()==True:
        return data
    else:
        return None

def fail_workers(pool, failures):
     """
     This function emulates failing nodes/processes by terminating the
     number of "failures" processes from the "pool".
     """
     if failures > pool._processes:
         raise Exception("You want to fail {} workers from a total of {}, but you can't!!".format(failures, pool._processes))

     ids = random.sample(range(pool._processes), failures)
     for i in ids:
         "emulating a worker fails via its terminate()"
         pool._pool[i].terminate()
         pool._pool[i].join()

     "after failing processes, we need to recover the amount of processes in the pool"
     pool._maintain_pool()


def choosePieces(size, recover, prefix):
     """
     Method to select a random stack of pieces to reconstruct
     the original data.

     size => Number of processes (number of file parts).
     recover => Number of minimum parts to recover the original data.
     prefix => Name of the pieces to choose
     """
     pieces = []
     while (len(pieces) < recover):
         if(len(pieces)== 0):
             pieces.append(random.randint(0, size-1))
         posiblePiece = random.randint(0, size-1)
         valid = True
         for x in range(len(pieces)):
             if (pieces[x] == posiblePiece):
                 valid = False
         if (valid):
             pieces.append(posiblePiece) # Random selection of pieces (Always minimum number of needed pieces)
     for i in range(len(pieces)):
        pieces[i] = prefix + '.p_' + str(pieces[i])
     return pieces


def test_reed_salomon(size, recover, prefix, decodedFile):
    """
    Method to do the test for the first part of the practical work
    here we can code the original into n pieces, and then we recover it.

    size => Number of processes (number of file parts).
    recover => Number of minimum parts to recover the original data.
    pieces => Array of pieces to recover the original data.
    testFile => A reasonable size file for testing.
    decodedFile => Name of the output file after the recovery
    """
    print "### Running test_reed_salomon with {} processes".format(size)
    p = multiprocessing.Pool(size)
    testFile = 'tmp/test'
    names = py_ecc.file_ecc.EncodeFile(testFile,prefix,size,recover)

    test.test_pool_who_i_am_uniform(size, names) # Assign a part of data to each process

    decList = []
    returnedData = p.map(worker, names)
    for i in range(recover):
        decList.append(returnedData[i])
    print decList

    py_ecc.file_ecc.DecodeFiles(decList,decodedFile) # Decode the file

    p.terminate()
    p.join()

    # Study
    fileSize = os.path.getsize(testFile) # File size
    print 'File size: ' + str(fileSize) + 'bits'

    pieceSize = os.path.getsize(prefix + '.p_' + str(0)) # Piece size
    print 'Piece size: ' + str(pieceSize) + 'bits'


def test_reed_salomon_elasticity(size, recover, fail, testFile, decodedFile):
     """
     Method to do the test for the second part of the practical work
     here we can distribute the pieces in different processes

     size => Number of processes (number of file parts).
     recover => Number of minimum parts to recover the original data.
     fail => Number of processes that we want fail
     testFile => A reasonable size file for testing.
     decodedFile => Name of the output file after the recovery
     """
     print "### Running test_reed_salomon_elasticity with {} processes".format(size)
     p = multiprocessing.Pool(size)
     prefix = testFile + '_backup'
     names = py_ecc.file_ecc.EncodeFile(testFile,prefix,size,recover)

     test.test_pool_who_i_am_uniform(size, names) # Assign a part of data to each process
     test.fail_workers(p, fail)

     for workers in p._pool:
        print workers.name
        if workers.is_alive():
            print 'uno vivo'
        else:
            print 'uno muerto'

     decList = []
     returnedData = p.map(worker, names)
     for i in range(len(names)):
        decList.append(returnedData[i])
     print decList
     py_ecc.file_ecc.DecodeFiles(decList,decodedFile) # Decode the file

     p.terminate()
     p.join()

     # Study
     fileSize = os.path.getsize(testFile) # File size
     print 'File size: ' + str(fileSize) + 'bits'

     pieceSize = os.path.getsize(prefix + '.p_' + str(0)) # Piece size
     print 'Piece size: ' + str(pieceSize) + 'bits'


if __name__ == "__main__":
    # First part of the practical work: Reed Salomon for storage

    test_reed_salomon(8, 2, 'tmp/testA1_backup', 'tmp/testA1.r') # first test with erasure codes 8,2
    test_reed_salomon(8, 4, 'tmp/testA2_backup', 'tmp/testA2.r') # second test with erasure codes 8,4
    test_reed_salomon(8, 7, 'tmp/testA3_backup', 'tmp/testA3.r') # third test with erasure codes 8,7

     # Second part of the practical work: Elasticity and Reed Salomon
    test_reed_salomon_elasticity(8, 7, 2, 'tmp/test', 'tmp/ls.r') # first test with erasure codes with elasticity
